{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Quizzes\n",
    "This notebook is for the computational/coding questions from lecture videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3 Quizzes\n",
    "\n",
    "### Quiz 3.7 Constrained Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(np.square([-2,-1,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 Quizzes\n",
    "### Quiz 4.8 Calculate MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1,2]) # [0,1,2] where 0 is bias\n",
    "x = np.array([[1,2],[2,3]])\n",
    "y = np.array([6,7])\n",
    "n = x.shape[0]\n",
    "MSE = (1/n) * np.sum(np.square(np.dot(w,x) - y))\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 4.9 Gradient Step Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2, 0.4, 1.6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1,2,3]) # augment [2,3] to [1,2,3]\n",
    "y = 6\n",
    "alpha = .1\n",
    "W = np.array([1,0,1])\n",
    "gradient= X.dot(X.dot(W) - y) # X*(yhat - y)\n",
    "W = W - (gradient*alpha) # Gradient update\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6 Quizzes\n",
    "### Quiz 6.2 Probability Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(AUB) == P(A) + P(B) True\n",
      "P(AUB) == P(A) + P(B) - P(A & B) True\n",
      "P(AUB) == P(A|B) False\n",
      "P(AUB) == P(A)*P(B) False\n"
     ]
    }
   ],
   "source": [
    "p_a = .4\n",
    "p_b = .6\n",
    "p_a_and_b = 0\n",
    "p_a_or_b = p_a + p_b - p_a_and_b\n",
    "print(f'P(AUB) == P(A) + P(B) {p_a + p_b == 1}')\n",
    "print(f'P(AUB) == P(A) + P(B) - P(A & B) {p_a + p_b == p_a_or_b}')\n",
    "print(f'P(AUB) == P(A|B) {p_a_and_b/p_b == 1}')\n",
    "print(f'P(AUB) == P(A)*P(B) {p_a*p_b == 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 6.3 Disjoing probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A) + P(B) - 0 == False\n"
     ]
    }
   ],
   "source": [
    "# Disjoint means P(A & B) == 0\n",
    "p_a = .6\n",
    "p_b = .7\n",
    "pa_and_b = 0\n",
    "print(f'P(A) + P(B) - 0 == { p_a+p_b-pa_and_b == 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 6.4 Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "# Confidence Intervals\n",
    "p = 1 - .2\n",
    "n = 1000\n",
    "z = 2\n",
    "# phat +/- z/2sqrt(n)\n",
    "wayOne = np.round(p + z/(2*np.sqrt(n)),2)\n",
    "# Standard error (1/n * .2 *.8)\n",
    "wayTwo = np.round(p + 2 * np.sqrt((1/n) *.2 *.8),2)\n",
    "print(wayOne)\n",
    "print(wayTwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 6.15 Text Classification and Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10\n",
      "P(Sports == Class) : 0.8\n",
      "P(word==Helment|Class==Sports): 0.3\n",
      "+1 smoothing: 0.2\n"
     ]
    }
   ],
   "source": [
    "data = ['Baseball','Helmet','Fans','Strike',\n",
    "       'Football','Helmet','Touchdown',\n",
    "       'Hoops',\n",
    "       'Motorcycle','Helmet','Light',\n",
    "       'Helmet','Ski']\n",
    "classes = ['Yes','Yes','Yes','No','Yes']\n",
    "\n",
    "# Q1: Vocab Size (unique count of words in training set)\n",
    "print(f'Vocab size: {len(set(data))}')\n",
    "\n",
    "# Q2: P(Class==Sports)\n",
    "sports = classes.count('Yes')/len(classes)\n",
    "print(f'P(Sports == Class) : {sports}')\n",
    "\n",
    "# Q3: Conditional Probabilties\n",
    "classes = ['Yes','Yes','Yes','Yes',\n",
    "          'Yes','Yes','Yes',\n",
    "          'Yes',\n",
    "          'No','No','No',\n",
    "          'Yes','Yes']\n",
    "dictt = {'words':data,'sports':classes}\n",
    "df = pd.DataFrame(data=dictt)\n",
    "\n",
    "# Numerator: Helment where helmet is present and class is sport\n",
    "#df[(df['words'] == 'Helmet') & (df['sports'] == 'Yes')]\n",
    "numerator = len(df[(df['words'] == 'Helmet') & (df['sports'] == 'Yes')])\n",
    "\n",
    "# Denominator: Words where class is sport\n",
    "#df[(df['sports'] == 'Yes')]\n",
    "denominator = len(df[(df['sports'] == 'Yes')])\n",
    "\n",
    "print(f'P(word==Helment|Class==Sports): {numerator/denominator}')\n",
    "\n",
    "# Q4: 1 + smoothing\n",
    "# Add vocab size to denominator and 1 to numerator\n",
    "\n",
    "print(f'+1 smoothing: {(3+1)/(10+10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 7 Quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.7310585786300049\n",
      "0.2689414213699951\n",
      "As x2 increases, p is below .5 (class 0). When x2 decreases, p is above .5 (class 1)\n"
     ]
    }
   ],
   "source": [
    "# Quiz 7.7\n",
    "import numpy as np\n",
    "g = lambda t: 1/(1 + np.exp(-t))\n",
    "w = np.array([6,0,-1])\n",
    "\n",
    "# dummy values for x\n",
    "x = np.array([1,0,6])\n",
    "\n",
    "# Remember that o(X*w.T + B) == o(t)\n",
    "# get probability of .5 when x2 == 6\n",
    "print(g(np.dot(w,x)))\n",
    "\n",
    "# x2 < 6\n",
    "x = np.array([1,0,5])\n",
    "\n",
    "# get probability when x2 < 6, p > .5 (class 1)\n",
    "print(g(np.dot(x,w)))\n",
    "\n",
    "# x2 > 6\n",
    "x = np.array([1,0,7])\n",
    "\n",
    "# get probability when x2 > 6, p < .5 (class 2)\n",
    "print(g(np.dot(x,w)))\n",
    "\n",
    "print('As x2 increases, p is below .5 (class 0). When x2 decreases, p is above .5 (class 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quiz 7.2 Normalized Distnace\n",
    "# 3x + 4y - 10 = 0\n",
    "# (0,0)\n",
    "\n",
    "(3*0 + 4*0 + 10)/np.sqrt(np.sum(np.square(np.array([3,4]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 7.8 Class Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class assigned: A\n"
     ]
    }
   ],
   "source": [
    "# Info from problem\n",
    "X = np.array([5,6])\n",
    "W = np.array([[2,3],[1,-5],[1,1]])\n",
    "b = np.array([4,10,12])\n",
    "\n",
    "# Probability function for multinomial\n",
    "p = lambda t: np.exp(t) / np.sum(np.exp(t))\n",
    "\n",
    "# Classes\n",
    "classes = ['A','B','C']\n",
    "\n",
    "# Calculate scores and probabilities\n",
    "scores = np.dot(X,W.T) + b\n",
    "probs = p(scores)\n",
    "\n",
    "# Print out class that the point will be assigned to\n",
    "print(f'Class assigned: {classes[np.argmax(probs)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 7.9 Negative Log Loss Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41772344946346773"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Info from problem (assume true target class is 0)\n",
    "perpDist = np.array([1.21,-.55,.15])\n",
    "\n",
    "# Since 1.21 shows the true target class for 0\n",
    "# The log loss calculation is only concerned with\n",
    "# that distance\n",
    "\n",
    "-np.log(np.exp(perpDist[0])/np.sum(np.exp(perpDist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 7.11 CXE (Binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.048587351573745"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Info from problem\n",
    "W = np.array([2,-1,4,1])\n",
    "X = np.array([1,5,1,2])\n",
    "y = 0\n",
    "p = lambda t: 1/(1 + np.exp(-t))\n",
    "prob = p(X.dot(W.T))\n",
    "CXE = -(y*np.log(prob) + (1-y) * np.log(1-prob))\n",
    "CXE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 7.12 Gradient Update (Binomial LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: [0.95257413 4.76287063 0.95257413 1.90514825]\n",
      "New weight vector: [ 1.99 -1.05  3.99  0.98]\n"
     ]
    }
   ],
   "source": [
    "# Info from problem\n",
    "W = np.array([2,-1,4,1])\n",
    "X = np.array([1,5,1,2])\n",
    "y = 0\n",
    "p = lambda t: 1/(1 + np.exp(-t))\n",
    "prob = p(X.dot(W.T))\n",
    "lr = .01\n",
    "\n",
    "gradient = np.dot((prob-y),X) # If you had more nested x values then 2/X.shape[0]\n",
    "print(f'Gradient: {gradient}')\n",
    "\n",
    "W = W - (lr * gradient)\n",
    "print(f'New weight vector: {np.round(W,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When t == 0, then p is: 0.5\n",
      "When t < 0\n",
      "[0.47502081252106,\n",
      " 0.45016600268752216,\n",
      " 0.425557483188341,\n",
      " 0.401312339887548,\n",
      " 0.3775406687981454,\n",
      " 0.35434369377420455,\n",
      " 0.3318122278318339,\n",
      " 0.31002551887238755,\n",
      " 0.289050497374996]\n",
      "When t > 0\n",
      "[0.52497918747894,\n",
      " 0.549833997312478,\n",
      " 0.574442516811659,\n",
      " 0.598687660112452,\n",
      " 0.6224593312018546,\n",
      " 0.6456563062257954,\n",
      " 0.6681877721681662,\n",
      " 0.6899744811276125,\n",
      " 0.7109495026250039]\n"
     ]
    }
   ],
   "source": [
    "# Binomial probability function for logit\n",
    "f = lambda t: 1/(1 + np.exp(-t))\n",
    "\n",
    "# Create probabilities with neg t's\n",
    "neg_p = [f((i*-1)/10) for i in range(0,11)]\n",
    "\n",
    "# Create probabilities with pos t's\n",
    "pos_p = [f(i/10) for i in range(0,11)]\n",
    "\n",
    "# Quick analysis between t and p\n",
    "print(f'When t == 0, then p is: {pos_p[0]}')\n",
    "print('When t < 0')\n",
    "pp.pprint(neg_p[1:10])\n",
    "print('When t > 0')\n",
    "pp.pprint(pos_p[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.74001823e+02, 1.40009115e+01, 3.33000911e+02, 9.11466454e-04,\n",
       "       1.48001823e+02])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playing around with multinomial regression\n",
    "rn.seed(42)\n",
    "X = np.array([rn.randint(-10,97) for i in range(5)])\n",
    "W = np.array([(rn.randint(0,4),rn.randint(10,10),\n",
    "               rn.randint(9,18),rn.randint(2,4),\n",
    "               rn.randint(9,10)) for i in range(5)])\n",
    "y = np.array([rn.randint(0,5) for i in range(5)])\n",
    "\n",
    "# Weighted distance\n",
    "dists = X.dot(W.T)\n",
    "\n",
    "# Calculate probabilities\n",
    "p = lambda t: np.exp(t) / np.sum(np.exp(t))\n",
    "probs = p(dists)\n",
    "\n",
    "# chosen class/probability\n",
    "yhat = y[np.argmax(probs)]\n",
    "\n",
    "# CXE\n",
    "cxe = -y*np.log(probs)-(1-y)*np.log(1-probs)\n",
    "cxe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding from assignments\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up parameters ######################################\n",
    "n_neighbors_range = list(range(1,6))\n",
    "p_range = list(range(1,4))\n",
    "###########################################################\n",
    "\n",
    "## Set-up pipe-line and gridsearch ########################\n",
    "pipe = Pipeline([\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('knn',knn_reg)\n",
    "])\n",
    "\n",
    "# ORDER: pipe, parms, just tag the rest\n",
    "knn_reg_gs = GridSearchCV(pipe,parameters,\n",
    "                          cv=3,n_jobs=-1,\n",
    "                          scoring=\"neg_mean_squared_error\",\n",
    "                          refit=True,verbose=1,n_jobs=-1)\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Batch GD Update ############\n",
    "def step(x, eta):\n",
    "    update = eta * ((4*x) + 4)\n",
    "    return x - update\n",
    "step(x=2, eta=.1)\n",
    "###############################\n",
    "\n",
    "## Sochastic GD Update ##################################\n",
    "X= [1,2,3]\n",
    "W = [1, 0, 1]\n",
    "y=[6]\n",
    "alpha= 0.1\n",
    "gradient= alpha*((np.array(X).dot(np.array(W)) - y) * X)\n",
    "newW= W - gradient\n",
    "print(newW)\n",
    "#########################################################\n",
    "\n",
    "## LG GD Update################# \n",
    "X=np.array([1, 5, 1, 2])\n",
    "w=np.array([2,-1, 4, 8])\n",
    "eta = .01\n",
    "y = 6\n",
    "w = w-eta*X.dot((X.dot(w) - y))\n",
    "print(np.round(w,3))\n",
    "################################\n",
    "\n",
    "## Create a pipeline###################\n",
    "pipe = Pipeline([\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('lineareg',LinearRegression())\n",
    "    ])\n",
    "#######################################\n",
    "\n",
    "## Calculate the STD Results########################################\n",
    "table.iloc[s,(c*2)+1] = np.round(np.std(results[count]), decimals=3)\n",
    "####################################################################\n",
    "\n",
    "## Selecting K-best features #####################\n",
    "Kbest = SelectKBest(f_regression,k=5)\n",
    "X_train_sel = Kbest.fit_transform(X_train,y_train)\n",
    "##################################################\n",
    "\n",
    "## Calculate MAPE####################################################\n",
    "mape = np.round(mean_absolute_percentage_error(y_val, y_val_pred), 3)\n",
    "#####################################################################\n",
    "\n",
    "## Separate x,y data and perform split training on training data ####\n",
    "X = train.drop(['count','registered','casual'],axis=1)\n",
    "y_reg, y_cas = train['registered'], train['casual']\n",
    "#####################################################################\n",
    "\n",
    "## Fit select-best features #########################################\n",
    "Kbest_reg = Kbest_REG.fit(X_train,y_train_reg)\n",
    "Kbest_cas = Kbest_CAS.fit(X_train,y_train_cas)\n",
    "#####################################################################\n",
    "\n",
    "## Calculate rsme ###################################################\n",
    "rmse = np.round(np.sqrt(mean_squared_error(y_val_reg,y_pred_reg)), 3)\n",
    "#####################################################################\n",
    "\n",
    "# Transform data to log #\n",
    "y = np.log(y + 1)\n",
    "#########################\n",
    "\n",
    "# Set-up sequential background selection #######\n",
    "lr = LinearRegression()\n",
    "sbs = SBS(lr,1,scoring=explained_variance_score)\n",
    "################################################\n",
    "\n",
    "# Pipeline with SBS ############################\n",
    "pipe = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('sbs',SBSFeatureSelector(sbs.subsets_,5)),\n",
    "    ('linreg',LinearRegression())\n",
    "])\n",
    "################################################\n",
    "\n",
    "# Convert log back to non-log values ##################\n",
    "y_pred = np.round((np.exp(y_pred_log) - 1).astype(int))\n",
    "#######################################################\n",
    "\n",
    "# Pipeline with multiple esimators #################################\n",
    "pipe = Pipeline([\n",
    "        ('scaler',StandardScaler()),\n",
    "        (estimator)\n",
    "])\n",
    "gs = GridSearchCV(pipe,params,cv=5,scoring=\"neg_mean_squared_error\")\n",
    "####################################################################\n",
    "\n",
    "# Closed form linear regression #################################\n",
    "w = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "#################################################################\n",
    "\n",
    "## Calculate MSE #############\n",
    "yhat = X.dot(w)\n",
    "MSE = np.square(yhat-y).mean()\n",
    "##############################\n",
    "\n",
    "## Make-pipeline with SelectPercentile + Poly ########################\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SelectPercentile(score_func=f_regression),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())\n",
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "              'selectpercentile__percentile' : [10,20,50,65,70,80,90]\n",
    "             }\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unnormalized perp dist #\n",
    "scores = (np.dot(x,W) + b)\n",
    "###########################\n",
    "\n",
    "## Binary Log Loss ############################################################################\n",
    "(-1/predicted.shape[0] * np.sum(actual * np.log(predicted) + (1-actual) * np.log(1-predicted)))\n",
    "###############################################################################################\n",
    "\n",
    "## Multinomial Log Loss (CXE) ##################\n",
    "m = predictions.shape[0]\n",
    "cxe = -1/m*np.sum(targets * np.log(predictions))\n",
    "################################################\n",
    "\n",
    "## GD Update (Binary) #######################\n",
    "perpDist= np.dot(X,w.T)\n",
    "p = 1 / (1 + np.exp(-perpDist)) # sigmoid\n",
    "gradient = (1/X.shape[0]) * np.dot((p - y),X)\n",
    "lr = 0.1\n",
    "w = w - (gradient * lr)\n",
    "#############################################\n",
    "\n",
    "## Splitting up training/test data with sub_sample ######################################################################\n",
    "X_train, _, y_train, _ = train_test_split(X_train, y_train, stratify=y_train, train_size=subsample_rate, random_state=42)\n",
    "X_test, _, y_test, _ = train_test_split(X_test,y_test,stratify=y_test,train_size=subsample_rate,random_state=42) \n",
    "#########################################################################################################################\n",
    "\n",
    "## Setting up k-fold and cross validation ##################################\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\n",
    "# set cv_results with scoring=scoring variable (which is set to 'accuracy')\n",
    "cv_results = cross_val_score(model, X, Y, cv=kfold,scoring=scoring) \n",
    "############################################################################\n",
    "\n",
    "## Pipeline with count vectors and tfidf #######\n",
    "pipeline =  Pipeline([\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',SGDClassifier(loss='log',max_iter=5))\n",
    "])\n",
    "################################################\n",
    "\n",
    "## Set params, grids, and pipeline ###########################\n",
    "scoring='precision_macro'\n",
    "# select handful of parameters to explore\n",
    "parameters = {'vect__ngram_range': ((1,1),(1,2)),\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf__alpha': (.001, .0001), \n",
    "                 'clf__penalty': ('l1', 'l2', 'elasticnet'), \n",
    "              # please explore different regularization terms \n",
    "              #('l1', 'l2', 'elasticnet'),\n",
    "                 'clf__l1_ratio': (.15,  .50)}\n",
    "grid_search = GridSearchCV(pipeline,parameters,\n",
    "                           n_jobs=-1,verbose=1,\n",
    "                           cv=3,scoring=scoring)  \n",
    "###############################################################\n",
    "\n",
    "## Logistic regression pipeline ########################\n",
    "pipe = Pipeline([\n",
    "     ('scaler',StandardScaler()),\n",
    "     ('lasso',LogisticRegression(penalty=\"l1\",C=param)),\n",
    "    ])  \n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labs\n",
    "## Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data ###################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.5, random_state=4)\n",
    "\n",
    "# simple knn predictor ###################\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(sepal_train, y_train)\n",
    "preds = knn.predict(sepal_test)\n",
    "\n",
    "# homegrown knn ################################################################\n",
    "# calculate euclidean distance\n",
    "def metric_func(x1, x2):\n",
    "    distance = np.linalg.norm(x1 - x2, ord=2, axis=1)\n",
    "    return distance\n",
    "\n",
    "examples = sepal_test[class_change]\n",
    "nearest_indices = np.zeros(shape=(examples.shape[0], 5), dtype=np.int) - 1\n",
    "nearest_distances = np.zeros(shape=(examples.shape[0], 5), dtype=np.float) - 1\n",
    "\n",
    "# collect index and distance for 5 nearest neighbors\n",
    "for i in range(examples.shape[0]):\n",
    "    distances = metric_func(examples[i], sepal_train)\n",
    "    index_order = np.argsort(distances)[:5]\n",
    "    nearest_indices[i] = index_order\n",
    "    nearest_distances[i] = distances[index_order]\n",
    "\n",
    "# Creating KNN object and calculating a list of cross val scores #\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Also cross val score with logistic regression ###############\n",
    "cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean()\n",
    "\n",
    "# Sample with fold ######################################\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cross_val_score(knn, X_train, y_train, cv=kfold)\n",
    "\n",
    "# Naming parameters ###############################################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'meow__n_neighbors': list(range(1,21)),\n",
    "          'meow__weights': ['uniform', 'distance'],\n",
    "          'meow__p': list(range(1,11))}\n",
    "pipe = Pipeline([\n",
    "    ('kitty',StandardScaler()),\n",
    "    ('meow',KNeighborsClassifier())\n",
    "])\n",
    "knn_gs = GridSearchCV(pipe, params, n_jobs=-1, cv=5, verbose=1)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "\n",
    "# HINT: use the parm names they give you, whatever your classifier is\n",
    "# do param_key == classifierObjectName__restofparams\n",
    "\n",
    "# Get best params #############\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "# Print out keys ####################\n",
    "print(grid_search.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 step gradient update: 0.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "# Perform one step of gradient descent\n",
    "f = lambda x : (4*x) + 4\n",
    "x1 = 2 - .1 * f(2)\n",
    "print(f'1 step gradient update: {x1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the minimum using GD:\n",
    "\n",
    "| &emsp;&emsp;$ Iteration $&emsp;&emsp;| &emsp;&emsp; __$x_i$__ &emsp;&emsp;| &emsp;&emsp;&emsp;$f'(x_i)= 4*x_i +4$ &emsp;&emsp;&emsp;&emsp; | &emsp;&emsp;&emsp;&emsp;&emsp; &emsp;&emsp; $Step Update = \\\\x_i - \\alpha * f'(x_i) $ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; |&emsp;&emsp; &emsp; $x_{i+i}$ &emsp;&emsp; &emsp; |\n",
    "|:--------:|:-----------------:|:-----------------:|:-----------------:|:-----------------:|\n",
    "|    1     | 2.0               |        $4*(2.0)+4$     |$2 - 0.1 * (4*(2.0)+4)$|      .7999        |\n",
    "| 2 | .7999 | $4*(.7999)+4$ | $.7999 - 0.1 * (4*(.7999)+4)$ |  .0799   |\n",
    "| 3 | .0799| $4*(.0799)+4$ |  $.0799 - 0.1 * (4*(.0799)+4)$ | -.3520 | \n",
    "|4 | -.3520| $4*(-.3520)+4$ | $-.3520 - 0.1 * (4*(-.3520)+4)$  | -.6112  |\n",
    "| 5 | -.6112|$4*(-.6112)+4$| $-.6112 - 0.1 * (4*(-.6112)+4)$ | -.7667 |\n",
    "| 6 | -.7667 | $4*(-.7667)+4$ | $-.7667 - 0.1 * (4*(-.7667)+4)$ | -.8600 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root is at:  -1.1428571666423404\n",
      "f(x) at root is:  -8.324819145144602e-08\n",
      "(-1.1428571666423404, [2, -3.5, 0.625, -2.46875, -0.1484375, -1.888671875, -0.58349609375, -1.5623779296875, -0.828216552734375, -1.3788375854492188, -0.9658718109130859, -1.2755961418151855, -1.0433028936386108, -1.2175228297710419, -1.0868578776717186, -1.184856591746211, -1.1113575561903417, -1.1664818328572437, -1.1251386253570672, -1.1561460309821996, -1.1328904767633503, -1.1503321424274873, -1.1372508931793845, -1.1470618301154616, -1.1397036274134038, -1.1452222794399471, -1.1410832904200396, -1.1441875321849704, -1.1418593508612722, -1.143605486854046, -1.1422958848594655, -1.1432780863554008, -1.1425414352334495, -1.143093923574913, -1.1426795573188153, -1.1429903320108885, -1.1427572509918338, -1.1429320617561247, -1.1428009536829062, -1.1428992847378203, -1.1428255364466349, -1.1428808476650238, -1.1428393642512322, -1.1428704768115758, -1.142847142391318, -1.1428646432065115, -1.1428515175951164, -1.1428613618036627, -1.142853978647253, -1.1428595160145603, -1.1428553629890799, -1.14285847775819, -1.1428561416813576, -1.1428578937389817, -1.1428565796957635, -1.1428575652281774, -1.1428568260788667, -1.14285738044085, -1.1428569646693625, -1.142857276497978, -1.1428570426265163, -1.1428572180301129, -1.1428570864774152, -1.1428571851419385, -1.1428571111435462, -1.1428571666423404])\n",
      "Root is at:  0.0\n",
      "f(x) at root is:  0.0\n",
      "Newton: 0.0\n",
      "Trace:[2.7, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Homegrown GD\n",
    "# is f(x) == 0, i.e., a root of f(x)\n",
    "eta = .5\n",
    "import pdb\n",
    "\n",
    "def dx(f, x):\n",
    "    return abs(0-f(x))\n",
    "\n",
    "# find the zeros of the derivative function\n",
    "# REPEAT find the zeros of the derivative function\n",
    "\n",
    "eta = .5 # step size multiplier\n",
    "\n",
    "#df is not used. WHY?\n",
    "def gradientDescent(f, df, x0, e, print_res=True):\n",
    "    delta = dx(f, x0)\n",
    "    approximations = [x0]\n",
    "    i=0\n",
    "    while delta > e:\n",
    "        #pdb.set_trace()\n",
    "        x0 = x0 - eta * f(x0)\n",
    "        if (i%10==0):\n",
    "            approximations.append(x0)\n",
    "        delta = dx(f, x0)\n",
    "        #print(delta)\n",
    "    if print_res:\n",
    "        print ('Root is at: ', x0)\n",
    "        print ('f(x) at root is: ', f(x0))\n",
    "    return (x0, approximations)\n",
    "\n",
    "# f is some sort of gradient function\n",
    "f = lambda x: 3.5*x + 4\n",
    "print(gradientDescent(f,0,2,.0000001))\n",
    "\n",
    "# Newton's formula\n",
    "from scipy.optimize import newton\n",
    "from sklearn.utils.testing import assert_almost_equal\n",
    "\n",
    "objectiveFunction = lambda x:x**2 - 4\n",
    "f = lambda x : 2*x \n",
    "df = 2\n",
    "\n",
    "#zero = newton(f, 1.0, fprime=df, maxiter=1000)\n",
    "scipyRoot = newton(f, 2.7,  maxiter=100)  #vi scipy\n",
    "(rootHomeGrown, trace) = gradientDescent(f, df,  2.7, 1e-5)\n",
    "\n",
    "print(f'Newton: {rootHomeGrown}')\n",
    "print(f'Trace:{trace}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4 Code *Linear Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after first Full GD/Batch update: [1.08 0.08]\n",
      "Weights after second Full GD/Batch update: [0.936 0.096]\n",
      "Weights after first Stochastic GD update: [1.04 0.04]\n",
      "Weights after second Stochastic GD update: [1.14 0.07]\n",
      "Total loss of function: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Full Gradient Descent\n",
    "w = np.array([1,0])\n",
    "x = np.array([[1,1],[2,1],[3,1],[4,1],[5,1]])\n",
    "y = np.array([2,3,4,3,5])\n",
    "\n",
    "# Compute prediction == yhat\n",
    "yhat = x.dot(w)\n",
    "gradient = np.dot(yhat-y,x)\n",
    "\n",
    "# Compute rest of formula\n",
    "w = w - .1 * 2/x.shape[0] * gradient\n",
    "print(f'Weights after first Full GD/Batch update: {w}')\n",
    "\n",
    "# Second step gradient\n",
    "w = w - .1 * (2/x.shape[0]) * np.dot(x.dot(w) - y,x)\n",
    "# Same as w - .1 * (2/5 * (w.dot(x.T) - y).dot(x))\n",
    "print(f'Weights after second Full GD/Batch update: {w}')\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "w = np.array([1,0])\n",
    "x = np.array([[1,1],[3,1],[5,1],[4,1],[2,1]])\n",
    "y = np.array([2,4,5,3,3])\n",
    "\n",
    "yhat = x[0].dot(w)\n",
    "gradient = np.dot(yhat - y[0],x[0])\n",
    "w = w - .1 * (2/x.shape[0]) * gradient\n",
    "print(f'Weights after first Stochastic GD update: {w}')\n",
    "\n",
    "yhat = x[1].dot(w)\n",
    "gradient = np.dot(yhat - y[1],x[1])\n",
    "w = w - .1 * (2/x.shape[0]) * gradient\n",
    "print(f'Weights after second Stochastic GD update: {np.round(w,2)}')\n",
    "\n",
    "# Compute total loss\n",
    "w = np.array([1,0])\n",
    "x = np.array([[1,1],[2,1],[3,1],[4,1],[5,1]])\n",
    "y = np.array([2,3,4,3,5])\n",
    "\n",
    "yhat = x.dot(w)\n",
    "loss = 1/x.shape[0] * np.sum(np.square(yhat-y))\n",
    "print(f'Total loss of function: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y Actual</th>\n",
       "      <th>Y Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>27.531578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>32.4</td>\n",
       "      <td>30.796233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.6</td>\n",
       "      <td>14.931903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>22.8</td>\n",
       "      <td>24.292355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16.1</td>\n",
       "      <td>19.976771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.305321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>17.8</td>\n",
       "      <td>18.400940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.539295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>23.677344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>16.8</td>\n",
       "      <td>19.464453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y Actual  Y Predict\n",
       "0      23.6  27.531578\n",
       "1      32.4  30.796233\n",
       "2      13.6  14.931903\n",
       "3      22.8  24.292355\n",
       "4      16.1  19.976771\n",
       "5      20.0  23.305321\n",
       "6      17.8  18.400940\n",
       "7      14.0  13.539295\n",
       "8      19.6  23.677344\n",
       "9      16.8  19.464453"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Selection with KBest (just making a dataframe)\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "k_best = SelectKBest(f_regression)\n",
    "feat_scores = k_best.fit_transform(X_train,y_train)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results[\"F Score\"] = k_best.scores_\n",
    "results[\"P Value\"] = k_best.pvalues_\n",
    "results[\"Support\"] = k_best.get_support()\n",
    "results[\"Features\"] = X.columns\n",
    "\n",
    "results.sort_values(by='F Score',ascending=False)\n",
    "\n",
    "# Feature selection Kbest with pipeline\n",
    "pipeline = Pipeline([\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_regression)),\n",
    "        ('lineReg', LinearRegression())\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "results = pipeline.predict(X_test)\n",
    "df = pd.DataFrame({'Y Actual':y_test,'Y Predict':results})\n",
    "df.head(10)\n",
    "\n",
    "# Handling missing values\n",
    "\n",
    "# drop nulls across rows\n",
    "df.dropna(axis=0)\n",
    "\n",
    "# drop nulls across cols\n",
    "df.dropna(axis=1)\n",
    "\n",
    "# drop nan rows\n",
    "df.dropna(how='all')  \n",
    "\n",
    "# drop in specific columns\n",
    "df.dropna(subset=['C'])\n",
    "\n",
    "# Use a mean to input missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imr = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample lasso regression update: [ 1.3  -4.47  3.27  6.57]\n"
     ]
    }
   ],
   "source": [
    "# One sample lasso regression update\n",
    "x = np.array([1,5,1,2])\n",
    "w = np.array([2,-1,4,8])\n",
    "y = 10\n",
    "eta = .1\n",
    "alpha = .3\n",
    "\n",
    "gradient = np.dot(x.dot(w)-y,x)\n",
    "lasso = np.append(0,alpha*np.sign(w[1:]))\n",
    "w1 = w - eta * (gradient + lasso)\n",
    "w1 = np.round(w1,2)\n",
    "print(f'One sample lasso regression update: {w1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.1 , -11.47,   1.87,   3.77])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1,5,1,2])\n",
    "W = np.array([2,-1,4,8])\n",
    "y = 10\n",
    "lambda1 = .1\n",
    "alpha = .3\n",
    "\n",
    "Wn = W - alpha * np.transpose(X).dot(X.dot(W) - y)\n",
    "Wn[1:] -= alpha * lambda1 * np.sign(W[1:])\n",
    "np.round(Wn, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample ridge regression update: [ 1.3  -4.47  3.18  6.36]\n"
     ]
    }
   ],
   "source": [
    "# One sample ridge regression update\n",
    "x = np.array([1,5,1,2])\n",
    "w = np.array([2,-1,4,8])\n",
    "y = 10\n",
    "eta = .1\n",
    "alpha = .3\n",
    "\n",
    "gradient = np.dot(x.dot(w)-y,x)\n",
    "ridge = np.append(0,alpha*w[1:])\n",
    "w2 = w - eta * (gradient + ridge)\n",
    "w2 = np.round(w2,2)\n",
    "print(f'One sample ridge regression update: {w2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi elastic gradient update: [ 1.3  -4.47  3.27  6.57]\n"
     ]
    }
   ],
   "source": [
    "# One sample elastic net regression\n",
    "x = np.array([1,5,1,2])\n",
    "w = np.array([2,-1,4,8])\n",
    "y = 10\n",
    "eta = .1\n",
    "alpha = .3\n",
    "r = 1\n",
    "\n",
    "gradient = np.dot(x.dot(w)-y,x)\n",
    "lasso = np.append(0,alpha*r*np.sign(w[1:]))\n",
    "ridge = np.append(0,alpha*((1-r)) * w[1:])\n",
    "w3 = w - eta * (gradient + lasso + ridge)\n",
    "w3 = np.round(w3,2)\n",
    "print(f'Multi elastic gradient update: {w3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi lasso gradient update: [ 1.75 -1.9   3.49  7.55]\n"
     ]
    }
   ],
   "source": [
    "# Multiple lasso regression update\n",
    "X = np.array([[1,5,1,2],[1,1,0,1],[1,4,3,2]])\n",
    "w = np.array([2,-1,4,8])\n",
    "y = np.array([10,1,3])\n",
    "eta = .01\n",
    "alpha = .1\n",
    "\n",
    "gradient = np.dot(np.dot(X,w)-y,X)*(2/X.shape[0])\n",
    "lasso = np.append(0,alpha*np.sign(w[1:]))\n",
    "w1 = w - eta * (gradient + lasso)\n",
    "w1 = np.round(w1,2)\n",
    "print(f'Multi lasso gradient update: {w1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi ridge gradient update: [ 1.75 -1.9   3.49  7.54]\n"
     ]
    }
   ],
   "source": [
    "# Multi ridge regression update\n",
    "X = np.array([[1,5,1,2],[1,1,0,1],[1,4,3,2]])\n",
    "w = np.array([2,-1,4,8])\n",
    "y = np.array([10,1,3])\n",
    "eta = .01\n",
    "alpha = .1\n",
    "\n",
    "gradient = np.dot(np.dot(X,w)-y,X)*(2/X.shape[0])\n",
    "ridge = np.append(0,alpha * w[1:])\n",
    "w2 = w - eta * (gradient + ridge)\n",
    "w2 = np.round(w2,2)\n",
    "print(f'Multi ridge gradient update: {w2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi elastic gradient update: [ 1.75 -1.9   3.49  7.54]\n"
     ]
    }
   ],
   "source": [
    "# Multi elastic regression update\n",
    "# When r == 0 then == ridge\n",
    "# When r == 1 then == lasso\n",
    "\n",
    "X = np.array([[1,5,1,2],[1,1,0,1],[1,4,3,2]])\n",
    "w = np.array([2,-1,4,8])\n",
    "y = np.array([10,1,3])\n",
    "eta = .01\n",
    "alpha = .1\n",
    "r = 0\n",
    "\n",
    "gradient = (2/X.shape[0]) * np.dot(X.dot(w) - y,X)\n",
    "lasso = np.append(0,alpha*r*np.sign(w[1:]))\n",
    "ridge = np.append(0,alpha*((1-r)) * w[1:])\n",
    "w3 = w - eta * (gradient + lasso + ridge)\n",
    "w3 = np.round(w3,2)\n",
    "print(f'Multi elastic gradient update: {w3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "\n",
    "## Binomial  + Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary LR Prob: [0.95257413 0.26894142]\n",
      "Classes: [1, 0]\n",
      "Binary LR CXE: 0.18092451954598235\n",
      "Binary LR Gradient Update: [-2.01266638  0.98733362 -1.01266638  1.98733362]\n",
      "Multinominal LR Prob: [0.70538451 0.03511903 0.25949646]\n",
      "Selected class: 0\n",
      "Multinominal LR CXE: 2.0156788834348527\n",
      "Multinomial LR Gradient Update: [-1.62360838  1.37639162 -0.62360838  2.37639162]\n",
      "Log loss for class 0: 0.34901221676818633\n",
      "Log loss for class 1: 3.3490122167681866\n",
      "Log loss for class 2: 1.3490122167681862\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1,0])\n",
    "w = np.array([-2,1,-1,2])\n",
    "X = np.array([[1,2,1,2],[1,1,0,0]])\n",
    "lr = 0.1\n",
    "\n",
    "## Calculate probability for binary class\n",
    "perpDist = X.dot(w) # AKA scores\n",
    "p = 1/(1 + np.exp(-perpDist))\n",
    "print(f'Binary LR Prob: {p}')\n",
    "\n",
    "# Select classes\n",
    "# if p < .5 then 0 else 1\n",
    "print(f'Classes: {[1,0]}')\n",
    "\n",
    "## Compute log loss (CXE) for binary class\n",
    "CXE = (-1/X.shape[0]) * np.sum(y * np.log(p) + (1-y) * np.log(1-p))\n",
    "print(f'Binary LR CXE: {CXE}')\n",
    "\n",
    "## Compute binary gradient update\n",
    "w  = w - lr * (1/X.shape[0]) * np.sum(np.dot((p-y),X))\n",
    "print(f'Binary LR Gradient Update: {w}')\n",
    "\n",
    "## Calculate probability for multinomial class\n",
    "y = np.array([0,1,2])\n",
    "w = np.array([-2,1,-1,2])\n",
    "X = np.array([[1,2,1,0],[2,0,0,0],[2,2,2,1]])\n",
    "lr = 0.1\n",
    "\n",
    "perpDist = X.dot(w)\n",
    "p = np.exp(perpDist)/np.sum(np.exp(perpDist))\n",
    "print(f'Multinominal LR Prob: {p}')\n",
    "\n",
    "# Select class\n",
    "yhat = y[np.argmax(p)]\n",
    "print(f'Selected class: {yhat}')\n",
    "\n",
    "# Compute log loss (CXE) for multinomial class\n",
    "CXE = (-1/X.shape[0]) * np.sum(y*np.log(p))\n",
    "print(f'Multinominal LR CXE: {CXE}')\n",
    "\n",
    "# Compute multinomial gradient update\n",
    "w = w - lr * (1/X.shape[0]) * np.sum(np.dot(p-y,X))\n",
    "print(f'Multinomial LR Gradient Update: {w}')\n",
    "\n",
    "# Loss loss for class 0\n",
    "print(f'Log loss for class 0: {-np.log(p[0])}')\n",
    "\n",
    "# Loss loss for class 1\n",
    "print(f'Log loss for class 1: {-np.log(p[1])}')\n",
    "\n",
    "# Loss loss for class 2\n",
    "print(f'Log loss for class 2: {-np.log(p[2])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Summaries\n",
    "## Module 2\n",
    "\n",
    "**KNN**:\n",
    "* Non-parametric method (doesn't conform to gaussian distributions + relies on continious data)\n",
    "* Used for regression + classification\n",
    "* Relies on K-closest neighbors/training examples in a feature space\n",
    "* Case-based (retrieves stored data and adapts based on new case)\n",
    "* Instance based (store all the training data)\n",
    "* Supervised learning (training data has labels)\n",
    "* Training phase == STORING DATA ONLY (labels and features)\n",
    "* Classification phase: k and unlabeled vector assigned a label, based on most frequent k training samples with respect to test point\n",
    "\n",
    "**KNN Regression**:\n",
    "\n",
    "Minimize the following function:\n",
    "    \\begin{equation}\n",
    "    MSE = \\frac{1}{n}\\Sigma (WX^i - y^i)^2\n",
    "    \\end{equation}\n",
    "    y is the *real value*\n",
    "\n",
    "**How to visualize it:**\n",
    "1. Fit a normal regression line\n",
    "2. Given an input x, draw a straight vertical line\n",
    "3. Find k-nearest neighbors based on distance (use one of the three distance metrics as listed below)\n",
    "4. Average the values of k-nearest neighbors to predict yhat\n",
    "\n",
    "**Distances**\n",
    "1. Euclidean\n",
    "2. Manhattan\n",
    "3. Minkowsi\n",
    "\n",
    "**Standardize Data**\n",
    "* Different features will have different scales, causing one to be a greater influence over the other. Scale the data to prevent this from happening\n",
    "* Will occur when there's a mixture of numerical and categorical variables\n",
    "\n",
    "\\begin{equation}\n",
    "X_i = \\frac{X-Min}{Max-Min}\n",
    "\\end{equation}\n",
    "\n",
    "**Decision Boundaries**\n",
    "* Voronoi Tesselation is a way to visualize decision boundaries with KNN\n",
    "\n",
    "**1-nearest neighbor classifier**\n",
    "* As training size increases, 1 nearest neighbor results in minimum achievable error rate given the distrubtion of the data\n",
    "\n",
    "**Complexity and generalization**\n",
    "* Limit model complexity\n",
    "* Split training and validation sets\n",
    "* Efficient Implementations\n",
    "    * Finding nearest neighbor is O(n)\n",
    "    * Binary search tree reduces to O(log n)\n",
    "    * Kd-tree (binary tree that sorts points on x then on y recursively)\n",
    "    * Parallel Hardware: Const if O(n) processors and O(log n) time\n",
    "    \n",
    "**Pros of KNN**\n",
    "* Fast training\n",
    "* little bias\n",
    "* Zero cost during learning unless\n",
    "    * Memorize the data\n",
    "    * Use kd trees\n",
    "\n",
    "**Cons of KNN**\n",
    "* Scaling\n",
    "* Sensitivity to noise and non-important features\n",
    "* Never used on image classificaton\n",
    "    * Terrible performance at test time\n",
    "    * distance metrics are not intuitive\n",
    "\n",
    "**Image classification**\n",
    "* Semantic gap: difference btwn descriptions of an object by different linguistic representations, like languages or symbols\n",
    "    * Example: converting an image into an array of digits\n",
    "* Challenges\n",
    "    * Viewpoint variations\n",
    "    * Illumination\n",
    "    * Deformation\n",
    "    * Occlusion\n",
    "    * Background clutter\n",
    "    * Variations\n",
    "    * No way to really \"code a cat\"\n",
    "* p/ord_p\n",
    "    * 1 == manhattan  distance (l1)\n",
    "    * 2 == euclidean_distance  (l2)\n",
    "    * arbitrary p == minkoski_distance (l_p)\n",
    "    * default is 2\n",
    "* Extra\n",
    "    * Training data size will cause classification speed to grow linearly\n",
    "    \n",
    "**Hyperparameter tuning**\n",
    "* Split data into k-folds, one fold is validation data used to tune hyperparameters\n",
    "* Average results from the fold\n",
    "\n",
    "## Module 3\n",
    "\n",
    "**What is optimization?**\n",
    "* The process of finding solutions that give a maximum or miminum of a function\n",
    "* Choses best avilable values of some objective function/domain\n",
    "* Optimal solution == where objective function reaches max or min\n",
    "* Global optimal is where no other feasible solutions with better values\n",
    "* min -(f(x)) == max(f(x))\n",
    "    * Optimization can mean minimization since max of a function is found by seeking minimum of the negative of the same function\n",
    "* Univariate/Multivariate optimization (Taxonomy)\n",
    "    Multivariable optimization problems\n",
    "    1. Uncontrained opt\n",
    "    2. Constrained opt\n",
    "       * Equality/Inequality constraints\n",
    "       \n",
    "**Unconstrained optimization**\n",
    "\n",
    "Example:\n",
    "\n",
    "\\begin{equation}\n",
    "Objective function = f(x) = x^3 - 12x + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Gradient function = f'(x) = 3x^2 - 12\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta gradient = f\"(x) = 6x\n",
    "\\end{equation}\n",
    "\n",
    "* Turning points \n",
    "    * Locate turning points in the gradient function (first derivative)\n",
    "    * Find x such that f(x) = 0\n",
    "    * Can solve analytically via closed form or use the quadratic formula\n",
    "    * Can solve numerically via gradient descent\n",
    "    * Newton's Method (numerical and analytical gradient)\n",
    "        * approximates f(x) == 0\n",
    "        * Uses f'(x) and f\"(x)\n",
    "        * Use f\"(x) to dtermine if candidate is max or min      \n",
    "        \n",
    " * Iterative methods (Newton)\n",
    "     * Start with x as candidate root\n",
    "     * Generate sequence of xn-1, xn, xn+1 which converges into solution\n",
    "     1. Initial guess\n",
    "     2. Approxmiate f(x) by tangent at (xi,f(xi)\n",
    "     3. Find where f_tangement_x0 = 0\n",
    "     4. Repeat until convergence\n",
    "     \n",
    "Example:\n",
    "\\begin{equation}\n",
    "f(x) = x^3 + 2x -4\n",
    "\\end{equation}\n",
    "   \n",
    "\\begin{equation}\n",
    "f\"(x) = 3x^2 + 2\n",
    "\\end{equation}\n",
    "\n",
    "Assuming 1 is first approximation of root, do one iteration of Newton-Raphson.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = 1^3 + 2(1) -4 = -1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f'(x) = 3(1)^2 + 2 = 5\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x_2 = x_i - \\frac{f(x_i)}{f'(x_i)} = 1 - \\frac{-1}{5} = 1.2\n",
    "\\end{equation}\n",
    "\n",
    "Convergence depends on good initial guess\n",
    "\n",
    "**Gradient Descent**\n",
    "\n",
    "Goal: Minimize f(x)\n",
    "\n",
    "Univariate case: \n",
    "\n",
    "\\begin{equation}\n",
    "x^{i+1} = x^i - \\alpha^i f'(x^i)\n",
    "\\end{equation}\n",
    "\n",
    "Multivariate case: \n",
    "\n",
    "\\begin{equation}\n",
    "x^{i+1} = x^i - \\alpha^i J'(x^i)\n",
    "\\end{equation}\n",
    "\n",
    "* Learning rates\n",
    "    * Fixed rate (with stochastic gradient descent)\n",
    "    * Dynamic, decreasing learning rate to 0 so converges to global minimum\n",
    "    * Or can be calculated\n",
    "    \n",
    "* Line Search\n",
    "    * A procedure that chooses alpha long a line\n",
    "    * f is minimized where gradient is othogonal to the search line\n",
    " \n",
    "\n",
    "* Second Order Necessary Condition: Necessary\n",
    "    * if f'(x) = 0, then it's necessary that f(x) is locally convex\n",
    "\n",
    "* First Order Condition and Second Order Necessary Condition are NOT Sufficient\n",
    "    * Doesn't distinguish btwn local mins and maxes or points of inflection\n",
    "    \n",
    "* Second Order Sufficient Condition\n",
    "    * If FOC and SOSC: f\"(x*) > 0, then x* is a local minimizer\n",
    "    \n",
    "**Unconstrained multivariate optimization**\n",
    "\n",
    "1. FOC: Gradient function is 0\n",
    "2. SOC: Check via Hessian is positive definite. Sufficient condition of extreme point to be a minimum is having a positive definite Hessian at such point.\n",
    "\n",
    "**Convexity**\n",
    "\n",
    "IFF every pair of points within the object and every point on a straight line is also in the object.\n",
    "\n",
    "**GD & ML**\n",
    "\n",
    "Use GD to find minimums of whatever objective function you are using (like MSE)\n",
    "\n",
    "* Linear Regression\n",
    "    * Minimize ||Xw-y||^2\n",
    "    \n",
    "* Classification\n",
    "    * Minimize log(1+exp(-yx^Tw))\n",
    "    \n",
    "## Module 4\n",
    "\n",
    "**Linear Regression Basics**\n",
    "\n",
    "* Supervised learning\n",
    "* Dependence of y on x (inputs) is linear\n",
    "* Model is: Y = B0 + B1*X + irreducible error\n",
    "* Predict yhat = B0_hat + B1_hat*x\n",
    "* Goal: Minimize Residual Squared Errors (y-yhat)^2\n",
    "\n",
    "**Assessing Accuracy**\n",
    "\n",
    "* Standard error reflects how the estimator varies under repeated sampling\n",
    "* Confidence intervals: give the probability the range contains a true value of the parameter\n",
    "    * AKA a 95% change the interval contains the true value\n",
    "* Hypothesis Testing\n",
    "    * H0: No relationship btwn X and Y (B0 = 0)\n",
    "    * H1: There's a relationship btwn X and Y (B0 != 0)\n",
    "* R-squared (variance explained) = 1-RSS/TSS\n",
    "* TSS == total summed squares == np.sum(np.square(y-yhat))\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "* Y = B0 + B1*x1 + B2*x2...+BP*xp + error\n",
    "* Best case\n",
    "    * Predictors are uncorrelated (balanced design)\n",
    "    * Each coefficient is tested separately\n",
    "    * Freeze all other constants while testing is possible\n",
    "* Issues\n",
    "    * Correlations in predictors cause issues in interpretations and high variance\n",
    "    * Claims of causality should be avoided \n",
    "* Forward Selection\n",
    "    * Begin with null model that has an intercept but no predictors\n",
    "    * Add the variables with the lowest RSS's\n",
    "    * Continue until a stopping rule is satisifed, like a p-value hitting a thershold\n",
    "* Backward Selection\n",
    "    * All variables are on the model\n",
    "    * Remove variable with largest p-value (least statistically sigificant)\n",
    "    * Continue until stopping threshold is reached\n",
    "\n",
    "This is the loss function aka Objective Function aka MSE: \n",
    "\n",
    "\\begin{equation}\\tag{1.3}\n",
    "f(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m}\\left[ \\boldsymbol{\\theta}\\cdot\\mathbf{x}'_i - y_i\\right]^2\n",
    "\\end{equation}\n",
    "*The multivariate case is denoted with a J rather than an F*\n",
    "\n",
    "Loss function is also written as:\n",
    "\n",
    "\\begin{equation}\\tag{1.3}\n",
    "LOSS_{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{n}\\sum_{i=1}^{n}\\left[ \\boldsymbol{\\theta}\\cdot\\mathbf{x}'_i - y_i\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "This is the formula used to compute the gradient (univariate):\n",
    "\n",
    "\\begin{equation}\\tag{3.1}\n",
    "\\nabla_{ LOSS_{MSE}}(\\boldsymbol{\\theta}) = \\frac{2}{n}\\,\\sum_{i=1}^{n}\\left[ \\boldsymbol{\\theta}\\cdot\\mathbf{x}'_i - y_i\\right] \\cdot \\mathbf{x}'\n",
    "\\end{equation}\n",
    "\n",
    "This is the closed form solution aka Normal Equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{\\theta}} = (X^T\\cdot{X})^{-1}\\cdot{X}^{T}\\cdot{y}\n",
    "\\end{equation}\n",
    "\n",
    "Not the best idea due to computational challenges for large data-sets\n",
    "\n",
    "**Breakdown of parameters**\n",
    "\n",
    "\\begin{equation}\\boldsymbol{\\theta}\\end{equation}\n",
    "The model's *parameter vector* where the bias term is the 0 index value. The rest are feature weights.\n",
    "\n",
    "\\begin{equation}\\boldsymbol{\\theta}^{T} \\end{equation}\n",
    "Same as what was said before, but transpose it.\n",
    "\n",
    "\\begin{equation}\\boldsymbol{x}\\end{equation}\n",
    "Feature vector\n",
    "\n",
    "\\begin{equation}\\boldsymbol{\\theta}^{T}\\cdot{x} \\end{equation}\n",
    "Dot product of the weights and input features...aka yhat == predictions\n",
    "\n",
    "**Numerical solution**\n",
    "\\begin{equation}\n",
    "lim\\frac{f(x+h)-f(x)}{h}\n",
    "\\end{equation}\n",
    "\n",
    "**How to compute the gradient**\n",
    "1. Initialize the parameters $\\theta$.\n",
    "2. Compute the gradient $\\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta})$.\n",
    "3. Update the parameters: $\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}) $  \n",
    "\n",
    "**Feature Selection**\n",
    "\n",
    "The process of choosing relevant features for our model. \n",
    "\n",
    "Why we do feature selection:\n",
    "* simplification of models to make them easier to interpret by researchers/users,\n",
    "* shorter training times,\n",
    "* to avoid the curse of dimensionality,\n",
    "* enhanced generalization by reducing overfitting (formally, reduction of variance).\n",
    "\n",
    "*Select K Best (features)*\n",
    "\n",
    "Select features according to the k highest scores. It takes as a parameter a score function, which must be applicable to a pair (X, y). The score function must return an array of scores, one for each feature X[:,i] of X. Small pvalue means the feature is independent of y. Large pvalue means it's non-randomly related to y, so it maybe important. \n",
    "\n",
    "**More formulas**\n",
    "\n",
    "$\n",
    "\\hat{y} = h_{\\mathbf{\\theta}}(\\mathbf{x}) = \\mathbf{\\theta}^T \\cdot \\mathbf{x}\n",
    "$\n",
    "\n",
    "\n",
    "**MSE cost function for a Linear Regression model**\n",
    "\n",
    "$\n",
    "\\text{MSE}(\\mathbf{X}, h_{\\mathbf{\\theta}}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{(\\mathbf{\\theta}^T \\cdot \\mathbf{x}^{(i)} - y^{(i)})^2}\n",
    "$\n",
    "\n",
    "\n",
    "**Normal Equation**\n",
    "\n",
    "$\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^T \\cdot \\mathbf{X})^{-1} \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}\n",
    "$\n",
    "\n",
    "\n",
    "**Partial derivatives notation :**\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j} \\text{MSE}(\\mathbf{\\theta})$\n",
    "\n",
    "\n",
    "**Partial derivatives of the cost function**\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial}{\\partial \\theta_j} \\text{MSE}(\\mathbf{\\theta}) = \\dfrac{2}{m}\\sum\\limits_{i=1}^{m}(\\mathbf{\\theta}^T \\cdot \\mathbf{x}^{(i)} - y^{(i)})\\, x_j^{(i)}\n",
    "$\n",
    "\n",
    "\n",
    "**Gradient vector of the cost function**\n",
    "\n",
    "$\n",
    "\\nabla_{\\mathbf{\\theta}}\\, \\text{MSE}(\\mathbf{\\theta}) =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial}{\\partial \\theta_0} \\text{MSE}(\\mathbf{\\theta}) \\\\\n",
    " \\frac{\\partial}{\\partial \\theta_1} \\text{MSE}(\\mathbf{\\theta}) \\\\\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial}{\\partial \\theta_n} \\text{MSE}(\\mathbf{\\theta})\n",
    "\\end{pmatrix}\n",
    " = \\dfrac{2}{m} \\mathbf{X}^T \\cdot (\\mathbf{X} \\cdot \\mathbf{\\theta} - \\mathbf{y})\n",
    "$\n",
    "\n",
    "\n",
    "**Gradient Descent step**\n",
    "\n",
    "$\n",
    "\\mathbf{\\theta}^{(\\text{next step})} = \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}}\\, \\text{MSE}(\\mathbf{\\theta})\n",
    "$\n",
    "\n",
    "\n",
    "$$\n",
    "J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\sum\\limits_{i=1}^{n}\\left| \\theta_i \\right|\n",
    "$$\n",
    "\n",
    "\n",
    "**Elastic Net cost function**\n",
    "\n",
    "$\n",
    "J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + r \\alpha \\sum\\limits_{i=1}^{m}\\left| \\theta_i \\right| + \\dfrac{1 - r}{2} \\alpha \\sum\\limits_{i=1}^{m}{\\theta_i^2}\n",
    "$\n",
    "\n",
    "\n",
    "** Lasso Regression cost function**\n",
    "\n",
    "$$\n",
    "J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\sum\\limits_{i=1}^{n}\\left| \\theta_i \\right|\n",
    "$$\n",
    "\n",
    "**Lasso Regression subgradient vector**\n",
    "\n",
    "$$\n",
    "g(\\mathbf{\\theta}, J) = \\nabla_{\\mathbf{\\theta}}\\, \\text{MSE}(\\mathbf{\\theta}) + \\alpha\n",
    "\\begin{pmatrix}\n",
    "  \\operatorname{sign}(\\theta_1) \\\\\n",
    "  \\operatorname{sign}(\\theta_2) \\\\\n",
    "  \\vdots \\\\\n",
    "  \\operatorname{sign}(\\theta_n) \\\\\n",
    "\\end{pmatrix} \\quad \\text{where } \\operatorname{sign}(\\theta_i) =\n",
    "\\begin{cases}\n",
    "-1 & \\text{if } \\theta_i < 0 \\\\\n",
    "0 & \\text{if } \\theta_i = 0 \\\\\n",
    "+1 & \\text{if } \\theta_i > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "**Gradient Descent step**\n",
    "\n",
    "$\n",
    "\\mathbf{\\theta}^{(\\text{next step})} = \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}}\\, \\text{MSE}(\\mathbf{\\theta})\n",
    "$\n",
    "\n",
    "**Flavors of gradient descent**\n",
    "* Stochastic\n",
    "    * Do gradient descent learning one example at a time (i.e., update the weight one training example at a time).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
